{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s0Ej_bXyQvnV"
   },
   "source": [
    "# Compute performance metrics for the given Y and Y_score without sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4CHb6NE7Qvnc"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook #purpose of import is just to check progress\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# other than these two you should not import any other packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KbsWXuDaQvnq"
   },
   "source": [
    "<pre>\n",
    "<font color='red'><b>A.</b></font> Compute performance metrics for the given data <strong>5_a.csv</strong>\n",
    "   <b>Note 1:</b> in this data you can see number of positive points >> number of negatives points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_a.csv</b>\n",
    "   <b>Note 3:</b> you need to derive the class labels from given score</pre> $y^{pred}= \\text{[0 if y_score < 0.5 else 1]}$\n",
    "\n",
    "<pre>\n",
    "<ol>\n",
    "<li> Compute Confusion Matrix </li>\n",
    "<li> Compute F1 Score </li>\n",
    "<li> Compute AUC Score, you need to compute different thresholds and for each threshold compute tpr,fpr and then use               numpy.trapz(tpr_array, fpr_array) <a href='https://stackoverflow.com/q/53603376/4084039'>https://stackoverflow.com/q/53603376/4084039</a>, <a href='https://stackoverflow.com/a/39678975/4084039'>https://stackoverflow.com/a/39678975/4084039</a> Note: it should be numpy.trapz(tpr_array, fpr_array) not numpy.trapz(fpr_array, tpr_array)</li>\n",
    "<li> Compute Accuracy Score </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook #purpose of import is just to check progress\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_a = pd.read_csv(\"5_a.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataset, y, threshold):\n",
    "    y_pred = []\n",
    "    for i in dataset[y]:\n",
    "        if i<threshold:\n",
    "            y_pred.append(0)\n",
    "        else:\n",
    "            y_pred.append(1)\n",
    "    return y_pred\n",
    "\n",
    "def compare_values(dataset,y,y_pred):\n",
    "    count_tp = 0\n",
    "    count_tn = 0\n",
    "    count_fp = 0\n",
    "    count_fn = 0\n",
    "    for i in range(len(dataset)):\n",
    "        if (dataset.y[i] == 0) and (dataset.y_pred[i] == 0):\n",
    "            count_tn+=1\n",
    "        if (dataset.y[i] == 0) and (dataset.y_pred[i] == 1):\n",
    "            count_fp+=1\n",
    "        if (dataset.y[i] == 1) and (dataset.y_pred[i] == 0):\n",
    "            count_fn+=1\n",
    "        if (dataset.y[i] == 1) and (dataset.y_pred[i] == 1):\n",
    "            count_tp+=1\n",
    "            \n",
    "    confusion_matrix = {\"tn\":count_tn, \"fn\":count_fn, \"fp\":count_fp, \"tp\":count_tp}\n",
    "    return confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_metrics(dataset):\n",
    "    y_list = list(dataset[\"y\"])\n",
    "    dataset[\"y_pred\"] = predict(dataset, \"proba\", 0.5)\n",
    "    confusion_matrix = compare_values(dataset, \"y\", \"y_pred\")\n",
    "    dataset.drop(columns = [\"y_pred\"])\n",
    "    dataset = dataset.sort_values(by = \"proba\", ascending = False)\n",
    "    positive_count = dataset.y.value_counts()[1]\n",
    "    precision = confusion_matrix[\"tp\"]/(confusion_matrix[\"tp\"]+confusion_matrix[\"fp\"])\n",
    "    recall = confusion_matrix[\"tp\"]/positive_count\n",
    "    F1 = 2*precision*recall/(precision+recall)\n",
    "    accuracy = (confusion_matrix[\"tp\"]+confusion_matrix[\"tn\"])/len(dataset)\n",
    "    #calculating AUC\n",
    "    negative_count = dataset.y.value_counts()[0]\n",
    "    tp_rate = []\n",
    "    fp_rate = []\n",
    "    for i in tqdm_notebook(dataset[\"proba\"]):\n",
    "        dataset[\"y_pred\"] = predict(dataset, \"proba\", i)\n",
    "        confusion_matrix_1 = compare_values(dataset, \"y\", \"y_pred\")\n",
    "        tp_rate.append(confusion_matrix_1[\"tp\"]/positive_count)\n",
    "        fp_rate.append(confusion_matrix_1[\"fp\"]/negative_count)\n",
    "    print(positive_count, negative_count)\n",
    "    auc = np.trapz(tp_rate, fp_rate)\n",
    "    \n",
    "    print(\"Confusion Matrix : \", confusion_matrix, \"\\nPrecision : \", precision, \"\\nRecall : \", recall, \"\\nF1 Score : \", F1, \"\\nAUC score : \", auc, \"\\nAccuracy : \", accuracy)\n",
    "    return confusion_matrix, precision, recall, F1, auc, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_a1 = dataset_a[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-82-e5d18e5d502b>:16: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm_notebook(dataset[\"proba\"]):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f1cc32b3294ba3a54e0f6b3ecb8803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10100.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10000 100\n",
      "Confusion Matrix :  {'tn': 0, 'fn': 0, 'fp': 100, 'tp': 10000} \n",
      "Precision :  0.9900990099009901 \n",
      "Recall :  1.0 \n",
      "F1 Score :  0.9950248756218906 \n",
      "AUC score :  0.48829900000000004 \n",
      "Accuracy :  0.9900990099009901\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix, precision, recall, F1, auc, accuracy = performance_metrics(dataset_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V5KZem1BQvn2"
   },
   "source": [
    "<pre>\n",
    "<font color='red'><b>B.</b></font> Compute performance metrics for the given data <strong>5_b.csv</strong>\n",
    "   <b>Note 1:</b> in this data you can see number of positive points << number of negatives points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_b.csv</b>\n",
    "   <b>Note 3:</b> you need to derive the class labels from given score</pre> $y^{pred}= \\text{[0 if y_score < 0.5 else 1]}$\n",
    "\n",
    "<pre>\n",
    "<ol>\n",
    "<li> Compute Confusion Matrix </li>\n",
    "<li> Compute F1 Score </li>\n",
    "<li> Compute AUC Score, you need to compute different thresholds and for each threshold compute tpr,fpr and then use               numpy.trapz(tpr_array, fpr_array) <a href='https://stackoverflow.com/q/53603376/4084039'>https://stackoverflow.com/q/53603376/4084039</a>, <a href='https://stackoverflow.com/a/39678975/4084039'>https://stackoverflow.com/a/39678975/4084039</a></li>\n",
    "<li> Compute Accuracy Score </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_b = pd.read_csv(\"5_b.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-58-e5d18e5d502b>:16: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm_notebook(dataset[\"proba\"]):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cbac32d57224f4098009bbdb152842a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10100.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100 10000\n",
      "Confusion Matrix :  {'tn': 9761, 'fn': 45, 'fp': 239, 'tp': 55} \n",
      "Precision :  0.1870748299319728 \n",
      "Recall :  0.55 \n",
      "F1 Score :  0.2791878172588833 \n",
      "AUC score :  0.9377570000000001 \n",
      "Accuracy :  0.9718811881188119\n",
      "Confusion Matrix :  {'tn': 9761, 'fn': 45, 'fp': 239, 'tp': 55} \n",
      "Precision :  0.1870748299319728 \n",
      "Recall :  0.55 \n",
      "F1 Score :  0.2791878172588833 \n",
      "AUC score :  0.9377570000000001 \n",
      "Accuracy :  0.9718811881188119\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix_b, precision_b, recall_b, F1_b, auc_b, accuracy_b = performance_metrics(dataset_b)\n",
    "print(\"Confusion Matrix : \", confusion_matrix_b, \"\\nPrecision : \", precision_b, \"\\nRecall : \", recall_b, \"\\nF1 Score : \", F1_b, \"\\nAUC score : \", auc_b, \"\\nAccuracy : \", accuracy_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GiPGonTzQvoB"
   },
   "source": [
    "<font color='red'><b>C.</b></font> Compute the best threshold (similarly to ROC curve computation) of probability which gives lowest values of metric <b>A</b> for the given data <strong>5_c.csv</strong>\n",
    "<br>\n",
    "\n",
    "you will be predicting label of a data points like this: $y^{pred}= \\text{[0 if y_score < threshold  else 1]}$\n",
    "\n",
    "$ A = 500 \\times \\text{number of false negative} + 100 \\times \\text{numebr of false positive}$\n",
    "\n",
    "<pre>\n",
    "   <b>Note 1:</b> in this data you can see number of negative points > number of positive points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_c.csv</b>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x5HIJzq1QvoE"
   },
   "outputs": [],
   "source": [
    "dataset_c = pd.read_csv(\"5_c.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimum_metrics(dataset):\n",
    "    y_list = list(dataset[\"y\"])\n",
    "    metric = {}\n",
    "    for i in tqdm_notebook(dataset[\"prob\"]):\n",
    "        dataset[\"y_pred\"] = predict(dataset, \"prob\", i)\n",
    "        confusion_matrix = compare_values(dataset, \"y\", \"y_pred\")\n",
    "        metric_value = 500*confusion_matrix[\"fn\"]+100*confusion_matrix[\"fp\"]\n",
    "        metric[i] = metric_value\n",
    "        dataset.drop(columns = [\"y_pred\"])\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-55-eab4da43e980>:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm_notebook(dataset[\"prob\"]):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc577fa545d24ff295a9387231e0a5e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2852.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_c = dataset_c.sort_values(by = \"prob\", ascending = False)\n",
    "result = minimum_metrics(dataset_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum value of specified metric is :  [0.2300390278970873] 141000\n"
     ]
    }
   ],
   "source": [
    "min_metric = min(result.values())\n",
    "min_key = [key for key in result if result[key]==min_metric]\n",
    "print(\"The minimum value of specified metric is : \", min_key, min_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sD4CcgjXQvoL"
   },
   "source": [
    "<pre>\n",
    "<font color='red'><b>D.</b></font> Compute performance metrics(for regression) for the given data <strong>5_d.csv</strong>\n",
    "    <b>Note 2:</b> use pandas or numpy to read the data from <b>5_d.csv</b>\n",
    "    <b>Note 1:</b> <b>5_d.csv</b> will having two columns Y and predicted_Y both are real valued features\n",
    "<ol>\n",
    "<li> Compute Mean Square Error </li>\n",
    "<li> Compute MAPE: https://www.youtube.com/watch?v=ly6ztgIkUxk</li>\n",
    "<li> Compute R^2 error: https://en.wikipedia.org/wiki/Coefficient_of_determination#Definitions </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_d = pd.read_csv(\"5_d.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(dataset, col1, col2):\n",
    "    error_list = []\n",
    "    for index, (value_1, value_2) in enumerate(zip(dataset[col1], dataset[col2])):\n",
    "        error_list.append(value_1 - value_2)\n",
    "    return error_list\n",
    "\n",
    "def ss_res(dataset, column):\n",
    "    ss_res = 0\n",
    "    for index, value in enumerate(dataset[column]):\n",
    "        ss_res = ss_res+value*value\n",
    "    return ss_res\n",
    "\n",
    "def mse(dataset, column):\n",
    "    return ss_res(dataset, column)/len(dataset[column])\n",
    "\n",
    "def abs_error(dataset, column):\n",
    "    abs_error_list = []\n",
    "    for index, value in enumerate(dataset[column]):\n",
    "        abs_error_list.append(abs(value))\n",
    "    return abs_error_list\n",
    "\n",
    "def mape(dataset, col1, col2):\n",
    "    mape = sum(dataset[col1])/sum(dataset[col2])\n",
    "    return mape\n",
    "\n",
    "def ss_tot(dataset, column):\n",
    "    ss_tot = 0\n",
    "    mean = dataset[\"y\"].mean()\n",
    "    for index, value in enumerate(dataset[column]):\n",
    "        ss_tot = ss_tot+(value-mean)**2\n",
    "    return ss_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error:  177.16569974554707 \n",
      "Mean Absolute Percentage Error:  0.1291202994009687 \n",
      "R Squared Error:  0.9563582786990964\n"
     ]
    }
   ],
   "source": [
    "dataset_d[\"error\"] = error(dataset_d, \"y\", \"pred\")\n",
    "dataset_d[\"abs_error\"] = abs_error(dataset_d, \"error\")\n",
    "MSE = mse(dataset_d, \"error\")\n",
    "MAPE = mape(dataset_d, \"abs_error\", \"y\")\n",
    "SS_RES = ss_res(dataset_d, \"error\")\n",
    "SS_TOT = ss_tot(dataset_d, \"y\")\n",
    "RSQE = 1-SS_RES/SS_TOT\n",
    "print(\"Mean Squared Error: \", MSE, \"\\nMean Absolute Percentage Error: \", MAPE, \"\\nR Squared Error: \", RSQE)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "5_Performance_metrics_Instructions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
